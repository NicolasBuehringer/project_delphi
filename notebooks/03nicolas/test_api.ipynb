{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1eaf281c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "73af3e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_n_days_ago(ndelta=0):\n",
    "    \"\"\"\n",
    "    Returns date n days ago in format YYYY_MM_DD. Without parameter it returns today's date.\n",
    "    \"\"\"\n",
    "    # get current time as datetime\n",
    "    current_time = datetime.datetime.today()\n",
    "\n",
    "    # check if a historic date is requested\n",
    "    if ndelta == 0:\n",
    "\n",
    "        current_time = str(current_time)\n",
    "\n",
    "        return f\"{current_time[:4]}_{current_time[5:7]}_{current_time[8:10]}\"\n",
    "\n",
    "    # get current time yesterday\n",
    "    current_time_yesterday = str(current_time - datetime.timedelta(ndelta))\n",
    "\n",
    "    return f\"{current_time_yesterday[:4]}_{current_time_yesterday[5:7]}_{current_time_yesterday[8:10]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ca8ff33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_url(keywords, start_date, end_date, max_results = 10):\n",
    "    \"\"\"\n",
    "    Create the url with current keywords\n",
    "    \"\"\"\n",
    "\n",
    "    #Change to the endpoint you want to collect data from\n",
    "    search_url = \"https://api.twitter.com/2/tweets/search/all\"\n",
    "\n",
    "    #change params based on the endpoint you are using\n",
    "    query_params = {'query': keywords,\n",
    "                    'start_time': start_date,\n",
    "                    'end_time': end_date,\n",
    "                    'max_results': max_results,\n",
    "                    'expansions': 'author_id,geo.place_id',\n",
    "                    'tweet.fields': 'id,text,created_at,lang,public_metrics,source',\n",
    "                    'user.fields': 'id,created_at,location,public_metrics',\n",
    "                    'place.fields': 'full_name,country_code,geo,place_type',\n",
    "                    'next_token': {}}\n",
    "\n",
    "    # return tuple: [0] is search_url and [1] is the dict with query params\n",
    "    return (search_url, query_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9429a273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_endpoint(url, headers, params, next_token):\n",
    "    \"\"\"\n",
    "    Returns the api response in a json format and sets 'next_token' value\n",
    "    \"\"\"\n",
    "\n",
    "    #params dict received from create_url function, set next_token value\n",
    "    params['next_token'] = next_token\n",
    "\n",
    "    # call api\n",
    "    response = requests.request(\"GET\", url, headers = headers, params = params)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "\n",
    "    # return api response as json\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00130568",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_api(headers,\n",
    "             keywords,\n",
    "             start_time,\n",
    "             end_time,\n",
    "             max_results,\n",
    "             new_token=None):\n",
    "    \"\"\"\n",
    "    Calls the twitter api and returns a dataframe containig all fetched information\n",
    "    \"\"\"\n",
    "    # sleep 4 seconds to not exceed api call limit per 15 Minutes (300)\n",
    "    time.sleep(4)\n",
    "\n",
    "    # create url by calling above defined function; url is a tuple\n",
    "    url = create_url(keywords, start_time, end_time, max_results)\n",
    "\n",
    "    # call api with above defined function\n",
    "    response = connect_to_endpoint(url[0], headers, url[1], new_token)\n",
    "\n",
    "    #response has 3 keys:\n",
    "    #\"data\": tweet information in a dict\n",
    "    #\"includes\": dict with one key \"users\" which is a dict of user information\n",
    "    #\"meta\": api request information\n",
    "\n",
    "\n",
    "    # create first DataFrame about tweets out of data key\n",
    "    tweet_df = pd.json_normalize(response[\"data\"])\n",
    "\n",
    "    # rename columns to not get identical names with user_df\n",
    "    tweet_df.rename(columns={'id':'tweet_id',\n",
    "                             'created_at': \"tweet_created_at\"},\n",
    "                    inplace=True)\n",
    "    print(tweet_df.iloc[1][\"tweet_created_at\"])\n",
    "    # create second DataFrame about users out of response key\n",
    "    user_df = pd.json_normalize(response[\"includes\"][\"users\"])\n",
    "\n",
    "    # rename columns to not get identical names with tweet_df\n",
    "    user_df.rename(columns={'id':'author_id',\n",
    "                            'created_at': \"profile_created_at\"},\n",
    "                    inplace=True)\n",
    "\n",
    "    # merge two dataframes into one\n",
    "    merged_df = tweet_df.merge(user_df, how = \"outer\", on=\"author_id\")\n",
    "\n",
    "\n",
    "    collected_tweets = response[\"meta\"][\"result_count\"]\n",
    "    next_token = response[\"meta\"].get(\"next_token\", False)\n",
    "\n",
    "    # return a df containing all information from the api call, the next_token for continuous search and number of collected tweets\n",
    "    return merged_df, next_token, collected_tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7d15198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_twitter(party, keywords, start_time,\n",
    "             end_time, max_results, tweet_amount):\n",
    "    \"\"\"\n",
    "    Searches twitter for a keywords searchstring until a maximum 'tweet_amount' has been reached or\n",
    "    there are no more results and next_token is False.\n",
    "    Returns a DataFrame containing all fetched information\n",
    "    \"\"\"\n",
    "\n",
    "    # create a main DataFrame to which all the api call results for one party are concatenated\n",
    "    one_party_df = pd.DataFrame()\n",
    "    headers = get_credentials()\n",
    "\n",
    "    # set collected tweet counter to zero\n",
    "    counter = 0\n",
    "\n",
    "    # call the api the first time resulting in a returned DataFrame, the next token and number of collected tweets\n",
    "    single_api_call_df, next_token, collected_tweets = call_api(headers, keywords,\n",
    "                                   start_time, end_time,\n",
    "                                   max_results,\n",
    "                                   new_token=None)\n",
    "\n",
    "    # concat the DataFrames to save function call result\n",
    "    one_party_df = pd.concat([one_party_df, single_api_call_df], ignore_index=True)\n",
    "\n",
    "    # add number of fetched tweets to counter\n",
    "    counter += collected_tweets\n",
    "\n",
    "    # while there is a next token in api call result\n",
    "    # call the api again with the next_token as additional parameter until the value becomes false\n",
    "    while next_token:\n",
    "\n",
    "\n",
    "        # call api with new next_token\n",
    "        single_api_call_df, next_token, collected_tweets = call_api(headers, keywords,\n",
    "                                       start_time, end_time,\n",
    "                                       max_results,\n",
    "                                       new_token=next_token)\n",
    "        \n",
    "        print(f\"Currently at party {party} and in total {counter} tweets\")\n",
    "        # save result and add number of tweets\n",
    "        one_party_df = pd.concat([one_party_df, single_api_call_df], ignore_index=True)\n",
    "        counter += collected_tweets\n",
    "\n",
    "\n",
    "        # break the loop if the predefined search limit is reached\n",
    "        if counter > tweet_amount:\n",
    "            print(f\"Reached predefined search limit of {tweet_amount} tweets\")\n",
    "            break\n",
    "\n",
    "    # after collecting all tweets create a new column containing the party's name\n",
    "    one_party_df[\"party\"] = party\n",
    "\n",
    "    # return a DataFrame with all tweets for one time period\n",
    "    return one_party_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b48201a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(start_time = False, end_time=False):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame containing all tweets for one day for 7 diffrent search keywords\n",
    "    for each party defined in query_dict\n",
    "    \"\"\"\n",
    "\n",
    "    # create a master DataFrame for all parties\n",
    "    all_parties_df = pd.DataFrame()\n",
    "\n",
    "    # set max results per api call\n",
    "    max_results = 500\n",
    "\n",
    "    if not (start_time and end_time):\n",
    "        # convert to ISO 8601: YYYY-MM-DDTHH:mm:sssZ\n",
    "        # this is UTC; we are not accounting for german time zone +02:00\n",
    "        start_time = f\"{get_date_n_days_ago(1).replace('_', '-')}T00:00:01.000Z\"\n",
    "        end_time = f\"{get_date_n_days_ago().replace('_', '-')}T00:00:01.000Z\"\n",
    "    else:\n",
    "        start_time = start_time\n",
    "        end_time = end_time\n",
    "\n",
    "    # for key, value in query_dict from twitter_api_params.py\n",
    "    for party, keywords in query_dict.items():\n",
    "\n",
    "        # set maximum tweet amount from keywords dict\n",
    "        tweet_amount =  keywords[1]\n",
    "\n",
    "        # collect the DataFrame for one party-keyword combination\n",
    "        one_party_df = search_twitter(party, keywords[0], start_time,\n",
    "                             end_time, max_results, tweet_amount)\n",
    "\n",
    "        # concat to master DataFrame\n",
    "        all_parties_df = pd.concat([all_parties_df, one_party_df], ignore_index=True)\n",
    "\n",
    "    # save master DataFrame as a csv\n",
    "    #all_parties_df.to_csv(\n",
    "    #    f\"temp_tweet_database_{str(current_time_yesterday)[5:7]}_{str(current_time_yesterday)[8:10]}.csv\"\n",
    "    #)\n",
    "\n",
    "\n",
    "    return all_parties_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "429f49d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-31T12:04:50.000Z\n",
      "2021-08-31T12:04:34.000Z\n",
      "2021-08-31T12:04:46.000Z\n",
      "2021-08-31T12:04:44.000Z\n",
      "2021-08-31T12:04:39.000Z\n",
      "2021-08-31T12:04:49.000Z\n",
      "2021-08-31T12:03:40.000Z\n"
     ]
    }
   ],
   "source": [
    "raw_data = get_data(\"2021-08-31T12:03:00.000Z\", \"2021-08-31T12:05:00.000Z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "61998145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105, 20)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a49f626f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['sentiment'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/34/1yvhb8vj085g3m5tpn1x91d80000gn/T/ipykernel_71297/857111445.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_clean_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/34/1yvhb8vj085g3m5tpn1x91d80000gn/T/ipykernel_71297/659795077.py\u001b[0m in \u001b[0;36mload_and_clean_csv\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Including only columns that we want to use in the future\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     df = df[['party',\n\u001b[0m\u001b[1;32m     18\u001b[0m                 \u001b[0;34m'tweet_date'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0;34m'author_id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/project_delphi/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3459\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3460\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3461\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3463\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/project_delphi/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1312\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1314\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m         if needs_i8_conversion(ax.dtype) or isinstance(\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/project_delphi/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis)\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1377\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['sentiment'] not in index\""
     ]
    }
   ],
   "source": [
    "test = load_and_clean_csv(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c01b831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_csv(df):\n",
    "    '''\n",
    "    Function loads DF data from the Twitter API+Sentiment and returns a cleaned DF\n",
    "    '''\n",
    "    # rename columns\n",
    "    df = df.rename(columns={\"tweet_created_at\": \"tweet_date\",\n",
    "                                \"public_metrics.retweet_count\": \"retweet_count\",\n",
    "                                \"public_metrics.reply_count\": \"reply_count\",\n",
    "                                \"public_metrics.like_count\": \"like_count\",\n",
    "                                \"profile_created_at\": \"profile_creation_date\",\n",
    "                                \"public_metrics.followers_count\": \"followers_count\",\n",
    "                                \"public_metrics.following_count\": \"following_count\",\n",
    "                                \"public_metrics.tweet_count\": \"user_tweet_count\"\n",
    "                                })\n",
    "\n",
    "    # Including only columns that we want to use in the future\n",
    "    df = df[['party',\n",
    "                'tweet_date',\n",
    "                'author_id',\n",
    "                'tweet_id',\n",
    "                'text',\n",
    "                'source',\n",
    "                'retweet_count',\n",
    "                'reply_count',\n",
    "                'like_count',\n",
    "                'profile_creation_date',\n",
    "                'followers_count',\n",
    "                'following_count',\n",
    "                'user_tweet_count',\n",
    "                'location',\n",
    "                'sentiment'\n",
    "                ]]\n",
    "\n",
    "    # Clean dataset columns:\n",
    "    # Change dtype\n",
    "    df[\"tweet_date\"] = df[\"tweet_date\"].astype(str)\n",
    "    df = df[df.tweet_date.str.match('(\\d{4}-\\d{2}-\\d{2}.\\d{2}:\\d{2}:\\d{2})')]\n",
    "    #df = df[(df.tweet_date.str.len() == 23) | (df.tweet_date.str.len() == 24)]\n",
    "    df['tweet_date'] = df['tweet_date'].str.slice(0,19)\n",
    "    df[\"tweet_date\"] = pd.to_datetime(df[\"tweet_date\"])\n",
    "    import ipdb\n",
    "    ipdb.set_trace()\n",
    "    df['profile_creation_date'] = df['profile_creation_date'].str.slice(0,19)\n",
    "    df[\"profile_creation_date\"] = pd.to_datetime(df[\"profile_creation_date\"])\n",
    "    # Drop duplicates\n",
    "    df = df.drop_duplicates()\n",
    "    # Transform sentiment to numeric type\n",
    "    dict_to_numeric = {\"negative\": -2, \"neutral\": 1, \"positive\": 2}\n",
    "    df[\"sentiment\"].replace(dict_to_numeric, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "62053289",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from os.path import join, dirname\n",
    "\n",
    "\n",
    "def get_credentials(headers):\n",
    "\n",
    "\n",
    "    # create authorization dict for the api\n",
    "    return headers\n",
    "\n",
    "# all queries:\n",
    "query_cdu = \"\"\"(@cducsubt OR @CDU OR @ArminLaschet  OR #Laschet OR #ArminLaschet  OR #arminlaschet OR #laschet OR #cdu OR #CDU OR CDU/CSU OR Laschet)\n",
    "lang:de -is:retweet\n",
    "-#GRUENEN -@Die_Gruenen -Baerbock -@ABaerbock\n",
    "-#SPD -@spdde -Scholz -@OlafScholz\n",
    "-#AFD -@AfD -Weidel -@Alice_Weidel -Chrupalla -@Tino_Chrupalla\n",
    "-#FDP -@fdp -Lindner -@c_lindner\n",
    "-#DieLinke -@dieLinke -Wissler -@Janine_Wissler -Bartsch -@DietmarBartsch\n",
    "-#FreieWaehler -@FREIEWAEHLER_BV\n",
    "-#diePARTEI -@DiePARTEI\n",
    "-@Tierschutzparte -NPD -@Piratenpartei -#Piraten -#dieBasis -@diebasispartei -#Volt -@VoltDeutschland\"\"\"\n",
    "\n",
    "query_linke = \"\"\"(@dieLinke OR @Janine_Wissler OR  @DietmarBartsch OR #DieLinke OR #DieLINKE OR #Linke OR #dielinke OR #Bartsch OR #Wissler OR Wissler OR DieLinke)\n",
    "lang:de -is:retweet\n",
    "-@cducsubt -@CDU -@ArminLaschet -#Laschet -#ArminLaschet -#laschet -#cdu -#CDU -CDU/CSU -Laschet\n",
    "-@Die_Gruenen -@ABaerbock -@GrueneBundestag -#Gruene -#Grünen -#Grüne -#GRUENEN -#AnnalenaBaerbock -#Baerbock -#baerbock -Baerbock -Grüne -Gruene\n",
    "-@spdde -@OlafScholz -@spdbt -#SPD -#spd -#Spd -#Scholz -#OlafScholz -#SCHOLZ -#scholz -Scholz -SPD\n",
    "-@AfD -@Alice_Weidel -@Tino_Chrupalla -#AFD -#AfD  -#afd  -#Weidel -#weidel -#Chrupalla -AFD -Weidel -Chrupalla\n",
    "-@fdp -@fdpbt -@c_lindner -#FDP -#fdp -#Fdp -#Lindner -#lindner -#LINDNER -#ChristianLindner -Lindner -FDP\n",
    "-#FreieWaehler -@FREIEWAEHLER_BV\n",
    "-#diePARTEI -@DiePARTEI\n",
    "-@Tierschutzparte -NPD -@Piratenpartei -#Piraten -#dieBasis -@diebasispartei -#Volt -@VoltDeutschland\"\"\"\n",
    "\n",
    "query_afd = \"\"\"( @AfD OR @Alice_Weidel OR @Tino_Chrupalla OR #AFD OR #AfD OR #afd OR #AlternativefürDeutschland OR #Weidel OR #weidel OR #AliceWeidel OR #WEIDEL OR #Chrupalla OR #chrupalla OR #TinoChruppala OR AFD OR Weidel OR Chrupalla)\n",
    "lang:de -is:retweet\n",
    "-@cducsubt -@CDU -@ArminLaschet -#Laschet -#ArminLaschet -#arminlaschet -#laschet -#cdu -#CDU -CDU/CSU -Laschet\n",
    "-@Die_Gruenen -@ABaerbock -@GrueneBundestag -#Gruene -#Grünen -#Grüne -#GRUENEN -#AnnalenaBaerbock -#Baerbock -#baerbock -Baerbock -Grüne -Gruene\n",
    "-@spdde -@OlafScholz -@spdbt -#SPD -#spd -#Spd -#Scholz -#OlafScholz -#SCHOLZ -#scholz -Scholz -SPD -Sozialdemokraten\n",
    "-#FDP -@fdp -Lindner -@c_lindner\n",
    "-#DieLinke -@dieLinke -Wissler -@Janine_Wissler -Bartsch -@DietmarBartsch\n",
    "-#FreieWaehler -@FREIEWAEHLER_BV\n",
    "-#diePARTEI -@DiePARTEI\n",
    "-@Tierschutzparte -NPD -@Piratenpartei -#Piraten -#dieBasis -@diebasispartei -#Volt -@VoltDeutschland\"\"\"\n",
    "\n",
    "query_fdp = \"\"\"(@fdp OR @fdpbt OR @c_lindner OR #FDP OR #fdp OR #Fdp OR #Lindner OR #lindner OR #LINDNER OR #ChristianLindner OR Lindner OR FDP)\n",
    "lang:de -is:retweet\n",
    "-@cducsubt -@CDU -@ArminLaschet -#Laschet -#ArminLaschet -#laschet -#cdu -#CDU -CDU/CSU -Laschet\n",
    "-@Die_Gruenen -@ABaerbock -@GrueneBundestag -#Gruene -#Grünen -#Grüne -#GRUENEN -#AnnalenaBaerbock -#Baerbock -#baerbock -Baerbock -Grüne -Gruene\n",
    "-@spdde -@OlafScholz -@spdbt -#SPD -#spd -#Spd -#Scholz -#OlafScholz -#SCHOLZ -#scholz -Scholz -SPD\n",
    "-@AfD -@Alice_Weidel -@Tino_Chrupalla -#AFD -#AfD -#afd -#Weidel -#weidel -#Chrupalla -AFD -Weidel -Chrupalla\n",
    "-#DieLinke -@dieLinke -Wissler -@Janine_Wissler -Bartsch -@DietmarBartsch\n",
    "-#FreieWaehler -@FREIEWAEHLER_BV\n",
    "-#diePARTEI -@DiePARTEI\n",
    "-@Tierschutzparte -NPD -@Piratenpartei -#Piraten -#dieBasis -@diebasispartei -#Volt -@VoltDeutschland\"\"\"\n",
    "\n",
    "query_others = \"\"\"(#FreieWaehler OR #FreieWähler OR @HubertAiwanger OR #FREIEWÄHLER OR @FREIEWAEHLER_BV #freiewähler2021 OR #diePARTEI OR @DiePARTEI OR @Tierschutzparte OR NPD OR @Piratenpartei OR #Piraten OR #dieBasis OR @diebasispartei OR #Volt OR @VoltDeutschland OR @oedp_de OR @bgepartei OR @TodenhoeferTeam OR #TeamTodenhoefer)\n",
    "lang:de -is:retweet\"\"\"\n",
    "\n",
    "query_spd = \"\"\"( @spdde OR @OlafScholz OR @spdbt OR #SPD OR #spd OR #Spd OR #Scholz OR #OlafScholz OR #SCHOLZ OR #scholz OR Scholz OR SPD OR Sozialdemokraten)\n",
    "lang:de -is:retweet\n",
    "-@cducsubt -@CDU -@ArminLaschet -#Laschet -#ArminLaschet -#arminlaschet -#laschet -#cdu -#CDU -CDU/CSU -Laschet\n",
    "-@Die_Gruenen -@ABaerbock -@GrueneBundestag -#Gruene -#Grünen -#Grüne -#GRUENEN -#AnnalenaBaerbock -#Baerbock -#baerbock -Baerbock -Grüne -Gruene\n",
    "-#AFD -@AfD -Weidel -@Alice_Weidel -Chrupalla -@Tino_Chrupalla\n",
    "-#FDP -@fdp -Lindner -@c_lindner\n",
    "-#DieLinke -@dieLinke -Wissler -@Janine_Wissler -Bartsch -@DietmarBartsch\n",
    "-#FreieWaehler -@FREIEWAEHLER_BV\n",
    "-#diePARTEI -@DiePARTEI\n",
    "-@Tierschutzparte -NPD -@Piratenpartei -#Piraten -#dieBasis -@diebasispartei -#Volt -@VoltDeutschland\"\"\"\n",
    "\n",
    "\n",
    "query_gruene = \"\"\"( @Die_Gruenen OR @ABaerbock OR @GrueneBundestag OR #Gruene OR #Grünen OR #Grüne OR #GRUENEN OR #AnnalenaBaerbock OR #Baerbock OR #baerbock OR Baerbock OR Grüne OR Gruene)\n",
    "lang:de -is:retweet\n",
    "-@cducsubt -@CDU -@ArminLaschet -#Laschet -#ArminLaschet -#arminlaschet -#laschet -#cdu -#CDU -CDU/CSU -Laschet\n",
    "-#SPD -@spdde -Scholz -@OlafScholz\n",
    "-#AFD -@AfD -Weidel -@Alice_Weidel -Chrupalla -@Tino_Chrupalla\n",
    "-#FDP -@fdp -Lindner -@c_lindner\n",
    "-#DieLinke -@dieLinke -Wissler -@Janine_Wissler -Bartsch -@DietmarBartsch\n",
    "-#FreieWaehler -@FREIEWAEHLER_BV\n",
    "-#diePARTEI -@DiePARTEI\n",
    "-@Tierschutzparte -NPD -@Piratenpartei -#Piraten -#dieBasis -@diebasispartei -#Volt -@VoltDeutschland\"\"\"\n",
    "\n",
    "# create a query dict to iterate over with the party name, its search string and the maximum number of tweets\n",
    "query_dict = {\n",
    "    \"SPD\": (query_spd, 100000),\n",
    "    \"AFD\": (query_afd, 50000),\n",
    "    \"CDU\": (query_cdu, 100000),\n",
    "    \"FDP\": (query_fdp, 50000),\n",
    "    \"GRUENE\": (query_gruene, 100000),\n",
    "    \"LINKE\": (query_linke, 40000),\n",
    "    \"OTHERS\": (query_others, 40000)\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
